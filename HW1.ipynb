{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этой программе я анализирую отзывы на Почту России. При 30-ти отзывах каждого типа уникальных слов было мало, так что я брала по 50 отзывов для тренировки и по 10 для проверки качества.  \n",
    "\n",
    "Плохие отзывы:  \n",
    "https://www.otzyvru.com/pochta-rossii?rating=1  \n",
    "Хорошие отзывы:  \n",
    "https://www.otzyvru.com/pochta-rossii?rating=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import time\n",
    "from nltk import word_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "from pprint import pprint\n",
    "from sklearn.metrics import accuracy_score\n",
    "from collections import Counter\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "session = requests.session()\n",
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# собирает список ссылок с одной страницы\n",
    "# возвращает список ссылок на полные отзывы\n",
    "def parse_page(link, number):\n",
    "    responce = session.get(link)\n",
    "    html = responce.text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    links = []\n",
    "    for h in soup.select('h2'):\n",
    "        if len(links) < number:\n",
    "            l = h.find('a').attrs['href']\n",
    "            links.append(l)\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# собирает список ссылок с обеих страниц\n",
    "# возвращает список ссылок на полные отзывы\n",
    "def parse_two_pages(link):\n",
    "    first_p = parse_page(link, 30)\n",
    "    second_p = parse_page(link+'&page=2', 20)\n",
    "    full_links = first_p + second_p   \n",
    "    return full_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# токенизация и начальная форма\n",
    "# возвращает список слов\n",
    "def process(text):\n",
    "    lem_text = []\n",
    "    for word in word_tokenize(text.lower()):\n",
    "        if word.isalpha():\n",
    "            result = morph.parse(word)[0]\n",
    "            lemma = result.normal_form\n",
    "            lem_text.append(lemma)\n",
    "    return lem_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# собирает текст отзывов\n",
    "# возвращает список всех слов отзывов этого типа\n",
    "def grab_reviews(links):\n",
    "    revs = []\n",
    "    for link in links:\n",
    "        responce = session.get(link)\n",
    "        html = responce.text\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        review = soup.find('span', {'class': 'comment description'}).text\n",
    "        lem_rev = process(review)\n",
    "        try:\n",
    "            revs += lem_rev\n",
    "        except:\n",
    "            print(revs)\n",
    "    return revs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# собираем ссылки на все полные отзывы\n",
    "good = 'https://www.otzyvru.com/pochta-rossii?rating=5'\n",
    "bad = 'https://www.otzyvru.com/pochta-rossii?rating=1'\n",
    "good_links = parse_two_pages(good)\n",
    "bad_links = parse_two_pages(bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# собираем списки всех хороших и плохих слов\n",
    "good_list = grab_reviews(good_links)\n",
    "bad_list = grab_reviews(bad_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# выкидываем пересекающиеся\n",
    "good_only = [g for g in good_list if g not in bad_list]\n",
    "bad_only = [b for b in bad_list if b not in good_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# собирает фреклист для одного списка слов\n",
    "def get_freqlist(word_list):\n",
    "    freqlist = Counter()\n",
    "    for word in word_list:\n",
    "        freqlist[word] += 1\n",
    "    return dict(freqlist.most_common(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# собираем хороший и плохой фреклист\n",
    "freqlists = {}\n",
    "good_freq = get_freqlist(good_only)\n",
    "bad_freq = get_freqlist(bad_only)\n",
    "freqlists['Это положительный отзыв'] = good_freq\n",
    "freqlists['Это отрицательный отзыв'] = bad_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# анализирует текст, выдаёт общий результат и красивенький\n",
    "def analyze(freqlists, text):\n",
    "    counts = Counter()\n",
    "    for sentiment, freqlist in freqlists.items():\n",
    "        freqlist = Counter(freqlist)\n",
    "        for word in text:\n",
    "            counts[sentiment] += int(freqlist[word] > 0)\n",
    "    result = counts.most_common()\n",
    "    nice_output = 'Вердикт: '+str(result[0][0])+' с шансами '+str(result[0][1])+' к '+str(result[1][1])\n",
    "    return result, nice_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вердикт: Это отрицательный отзыв с шансами 3 к 0\n"
     ]
    }
   ],
   "source": [
    "# тестируем\n",
    "test_text = 'Не прислали извещения, молча отправили обе посылки обратно в Китай'\n",
    "print(analyze(freqlists, process(test_text))[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# собирает 20 тестовых текстов\n",
    "# отличается от grab_reviews() тем, что делает подсписки, а не всё в кучу\n",
    "def grab_test(links):\n",
    "    revs = []\n",
    "    for link in links:\n",
    "        responce = session.get(link)\n",
    "        html = responce.text\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        review = soup.find('span', {'class': 'comment description'}).text\n",
    "        lem_rev = process(review)\n",
    "        revs.append(lem_rev)\n",
    "    return revs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# анализирует качество\n",
    "def test_detection(freqlists):\n",
    "    test_list = []\n",
    "    gold = ['Это положительный отзыв']*10+['Это отрицательный отзыв']*10\n",
    "    results = []\n",
    "    test_links = ['https://www.otzyvru.com/pochta-rossii?rating=5&page=3','https://www.otzyvru.com/pochta-rossii?rating=1&page=3']\n",
    "    for link in test_links:\n",
    "        test_list += grab_test(parse_page(link, 10))\n",
    "    for text in test_list:\n",
    "        predicted_sentiment = analyze(freqlists, text)[0][0][0]\n",
    "        results.append(predicted_sentiment)\n",
    "    print(\"Accuracy: %.4f\" % accuracy_score(results, gold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9000\n"
     ]
    }
   ],
   "source": [
    "test_detection(freqlists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Идеи улучшения программы:**  \n",
    "1. В списке уникальных слов из хороших отзывов почти все слова имеют положительную окраску, а в таком же списке из плохих отзывов оказывается очень много нейтральных слов (см. ниже). Я и так увеличила сет с 30 до 50 отзывов каждого типа, но, кажется, хорошо бы сделать его ещё побольше.  \n",
    "2. Мне не нравится, что функция grab_test() почти дублирует grab_reviews() за исключением другой структуры списка: [[раз, два], [три, четыре]] вместо [раз, два, три, четыре].\n",
    "3. После подсчёта accuracy ради интереса можно вывести те отзывы, где результат анализа не совпал с правильным."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Примеры хороших слов: \n",
      " приятный, нереальный, условие, ваш, труд, опс, стачка, грамотно, очевидно, отметить, вежливый, улыбаться, благодарить, компетентность, клиентоориентированность\n",
      "\n",
      "Примеры плохих слов: \n",
      " август, сделать, заказ, алиэкспресс, сей, пора, ни, прийти, отслеживание, прислать, заказ, доходить, ужасный, емs, ижевск\n"
     ]
    }
   ],
   "source": [
    "print('Примеры хороших слов: \\n', ', '.join(good_only[:15]))\n",
    "print('\\nПримеры плохих слов: \\n', ', '.join(bad_only[:15]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
