{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Русскоязычный текст (119 слов):  \n",
    "*Мария затопила **печь** и принялась замешивать тесто. Ей всегда нравилось **печь** пироги и **бейглы**, но, увы, это дело редко получалось хорошо. В прошлый раз **стекло** старой миски лопнуло и дало **течь**, и свежее тесто начало незаметно **течь** на скатерть, а потом и вовсе **стекло** на пол. В другой раз в печи было так жарко, что сгорел и пирог, и **жаркое** к ужину, пока Мария пила малиновый чай и любовалась на **жаркое** летнее солнце. **Временами** от готовки её отвлекал сосед, **что** любил жаловаться всем подряд, **что** с такими **временами** года урожай гибнет и работать в поле приходится всё больше. \"Если уж выбрал для жизни **село**, то будь готов работать с самого утра и пока солнце не **село**\" - отвечала ему Мария.*  \n",
    "\n",
    "Выделенные жирным слова представлют сложность для POS-теггера, потому что имеют грамматические омонимы. Некоторые слова различить довольно просто, если учитывать контекст вокруг них, а в некоторых случаях это не поможет, например: *пока солнце не село*. Сочетания типа сущ + не + сущ вполне возможны в русском языке. Также в тексте присутствует несловарное слово *бейглы*, которое парсерам придётся распознавать по аналогии (а сначала хорошо или не очень хорошо её подобрать).  \n",
    "#### Англоязычный текст (117 слов):  \n",
    "*State your name and date of birth for the **record**, please.  \n",
    "We need you to **record** more messages because your voice **sounds** more natural than **mine**.  \n",
    "They must drive **fast** to get to that magic **well** in time.  \n",
    "Several natural molecules perform a **fast** internal conversion.  \n",
    "Somebody told me he works at that **mine**.  \n",
    "Self-defense is also a constitutional **right** and fighting for it is the **right** thing to do.  \n",
    "I do not feel **well** now so I need to **lie** down and rest for a bit.  \n",
    "We are able to detect any **lie** you say, dear.  \n",
    "All I did was **light** a **match** and then everything exploded!  \n",
    "She picked my suit to **match** her wedding dress.*  \n",
    "\n",
    "Это уже не связный текст, а отдельные предложения, потому что я не смогла придумать так же хорошо, как на русском. Но тут тоже есть как более простые для определения омонимы, так и сложные случаи типа *a fast internal conversion*, где сочетание *a fast* можно расценить как существительное, или омоним *well*, крайне редко встречающийся в виде существительного. Выделены только некоторые потенциально сложные слова, за счёт бедной английской морфологии и помимо специально добавленных слов в тексте много возможностей для ошибки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import navec\n",
    "import nltk\n",
    "import spacy\n",
    "import re\n",
    "from navec import Navec\n",
    "from slovnet import NER\n",
    "from ipymarkup import show_span_ascii_markup as show_markup\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from pprint import pprint\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "from pymystem3 import Mystem\n",
    "from sklearn.metrics import accuracy_score\n",
    "m = MorphAnalyzer()\n",
    "mst = Mystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,\n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsSyntaxParser,\n",
    "    NewsNERTagger,\n",
    "    PER,\n",
    "    NamesExtractor,\n",
    "    Doc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmenter = Segmenter()\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "syntax_parser = NewsSyntaxParser(emb)\n",
    "ner_tagger = NewsNERTagger(emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Разметка текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# открыть тексты из файлов\n",
    "with open('test_rus.txt', encoding='UTF-8') as f:\n",
    "    test_rus = f.read()\n",
    "with open('test_eng.txt', encoding='UTF-8') as f:\n",
    "    test_eng = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Мария затопила печь и принялась замешивать тесто. Ей всегда нравилось печь пироги и бейглы, но, увы, это дело редко получалось хорошо. В прошлый раз стекло старой миски лопнуло и дало течь, и свежее тесто начало незаметно течь на стол, а потом и вовсе стекло на пол. В другой раз в печи было так жарко, что сгорел и пирог, и жаркое к ужину, пока Мария пила малиновый чай и любовалась на жаркое летнее солнце. Временами от готовки её отвлекал сосед, что любил жаловаться всем подряд, что с такими временами года урожай гибнет и работать в поле приходится всё больше. \"Если уж выбрал для жизни село, то будь готов работать с раннего утра и пока солнце не село\" - отвечала ему Мария.\n"
     ]
    }
   ],
   "source": [
    "print(test_rus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Мария\n",
      "NOUN\n",
      "затопила\n",
      "VERB\n",
      "печь\n",
      "NOUN\n",
      "и\n",
      "CONJ\n",
      "принялась\n",
      "VERB\n",
      "замешивать\n",
      "INFN\n",
      "тесто\n",
      "NOUN\n",
      ".\n",
      "PNKT\n",
      "Ей\n",
      "NPRO\n",
      "всегда\n",
      "ADVB\n",
      "нравилось\n",
      "VERB\n",
      "печь\n",
      "INFN\n",
      "пироги\n",
      "NOUN\n",
      "и\n",
      "CONJ\n",
      "торты\n",
      "NOUN\n",
      ",\n",
      "PNKT\n",
      "но\n",
      "CONJ\n",
      ",\n",
      "PNKT\n",
      "увы\n",
      "INTJ\n",
      ",\n",
      "PNKT\n",
      "это\n",
      "DONTKNOW\n",
      "дело\n",
      "NOUN\n",
      "редко\n",
      "ADVB\n",
      "получалось\n",
      "VERB\n",
      "хорошо\n",
      "ADVB\n",
      ".\n",
      "PNKT\n",
      "В\n",
      "PREP\n",
      "прошлый\n",
      "ADJF\n",
      "раз\n",
      "NOUN\n",
      "стекло\n",
      "NOUN\n",
      "старой\n",
      "ADJF\n",
      "миски\n",
      "NOUN\n",
      "лопнуло\n",
      "VERB\n",
      "и\n",
      "CONJ\n",
      "дало\n",
      "VERB\n",
      "течь\n",
      "NOUN\n",
      ",\n",
      "PNKT\n",
      "и\n",
      "CONJ\n",
      "свежее\n",
      "ADJF\n",
      "тесто\n",
      "NOUN\n",
      "начало\n",
      "VERB\n",
      "незаметно\n",
      "ADVB\n",
      "течь\n",
      "INFN\n",
      "на\n",
      "PREP\n",
      "стол\n",
      "NOUN\n",
      ",\n",
      "PNKT\n",
      "а\n",
      "CONJ\n",
      "потом\n",
      "ADVB\n",
      "и\n",
      "CONJ\n",
      "вовсе\n",
      "ADVB\n",
      "стекло\n",
      "VERB\n",
      "на\n",
      "PREP\n",
      "пол\n",
      "NOUN\n",
      ".\n",
      "PNKT\n",
      "В\n",
      "PREP\n",
      "другой\n",
      "ADJF\n",
      "раз\n",
      "NOUN\n",
      "в\n",
      "PREP\n",
      "печи\n",
      "NOUN\n",
      "было\n",
      "VERB\n",
      "так\n",
      "ADVB\n",
      "жарко\n",
      "ADVB\n",
      ",\n",
      "PNKT\n",
      "что\n",
      "CONJ\n",
      "сгорел\n",
      "VERB\n",
      "и\n",
      "CONJ\n",
      "пирог\n",
      "NOUN\n",
      ",\n",
      "PNKT\n",
      "и\n",
      "CONJ\n",
      "жаркое\n",
      "NOUN\n",
      "к\n",
      "PREP\n",
      "ужину\n",
      "NOUN\n",
      ",\n",
      "PNKT\n",
      "пока\n",
      "CONJ\n",
      "Мария\n",
      "NOUN\n",
      "пила\n",
      "VERB\n",
      "малиновый\n",
      "ADJF\n",
      "чай\n",
      "NOUN\n",
      "и\n",
      "CONJ\n",
      "любовалась\n",
      "VERB\n",
      "на\n",
      "PREP\n",
      "жаркое\n",
      "ADJF\n",
      "летнее\n",
      "ADJF\n",
      "солнце\n",
      "NOUN\n",
      ".\n",
      "PNKT\n",
      "Временами\n",
      "ADVB\n",
      "от\n",
      "PREP\n",
      "готовки\n",
      "NOUN\n",
      "её\n",
      "NPRO\n",
      "отвлекал\n",
      "VERB\n",
      "сосед\n",
      "NOUN\n",
      ",\n",
      "PNKT\n",
      "что\n",
      "RELPRO\n",
      "любил\n",
      "VERB\n",
      "жаловаться\n",
      "INFN\n",
      "всем\n",
      "NPRO\n",
      "подряд\n",
      "ADVB\n",
      ",\n",
      "PNKT\n",
      "что\n",
      "CONJ\n",
      "с\n",
      "PREP\n",
      "такими\n",
      "ADJF\n",
      "временами\n",
      "NOUN\n",
      "года\n",
      "NOUN\n",
      "урожай\n",
      "NOUN\n",
      "гибнет\n",
      "VERB\n",
      "и\n",
      "CONJ\n",
      "работать\n",
      "INFN\n",
      "в\n",
      "PREP\n",
      "поле\n",
      "NOUN\n",
      "приходится\n",
      "VERB\n",
      "всё\n",
      "ADVB\n",
      "больше\n",
      "COMP\n",
      ".\n",
      "PNKT\n",
      "``\n",
      "PNKT\n",
      "Если\n",
      "CONJ\n",
      "уж\n",
      "PRCL\n",
      "выбрал\n",
      "VERB\n",
      "для\n",
      "PREP\n",
      "жизни\n",
      "NOUN\n",
      "село\n",
      "NOUN\n",
      ",\n",
      "PNKT\n",
      "то\n",
      "CONJ\n",
      "будь\n",
      "VERB\n",
      "готов\n",
      "ADJS\n",
      "работать\n",
      "INFN\n",
      "с\n",
      "PREP\n",
      "самого\n",
      "ADJF\n",
      "утра\n",
      "NOUN\n",
      "и\n",
      "CONJ\n",
      "пока\n",
      "ADVB\n",
      "солнце\n",
      "NOUN\n",
      "не\n",
      "PRCL\n",
      "село\n",
      "VERB\n",
      "''\n",
      "PNKT\n",
      "-\n",
      "PNKT\n",
      "отвечала\n",
      "VERB\n",
      "ему\n",
      "NPRO\n",
      "Мария\n",
      "NOUN\n",
      ".\n",
      "PNKT\n"
     ]
    }
   ],
   "source": [
    "# разметить русский текст\n",
    "golden_rus = []\n",
    "for word in word_tokenize(test_rus):\n",
    "    print(word)\n",
    "    golden_rus.append(input())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NOUN', 'VERB', 'NOUN', 'CONJ', 'VERB', 'VERB', 'NOUN', 'PNKT', 'PRON', 'ADVB', 'VERB', 'VERB', 'NOUN', 'CONJ', 'NOUN', 'PNKT', 'CONJ', 'PNKT', 'INTJ', 'PNKT', 'PRON', 'NOUN', 'ADVB', 'VERB', 'ADVB', 'PNKT', 'PREP', 'ADJ', 'NOUN', 'NOUN', 'ADJ', 'NOUN', 'VERB', 'CONJ', 'VERB', 'NOUN', 'PNKT', 'CONJ', 'ADJ', 'NOUN', 'VERB', 'ADVB', 'VERB', 'PREP', 'NOUN', 'PNKT', 'CONJ', 'ADVB', 'CONJ', 'ADVB', 'VERB', 'PREP', 'NOUN', 'PNKT', 'PREP', 'ADJ', 'NOUN', 'PREP', 'NOUN', 'VERB', 'ADVB', 'ADVB', 'PNKT', 'CONJ', 'VERB', 'CONJ', 'NOUN', 'PNKT', 'CONJ', 'NOUN', 'PREP', 'NOUN', 'PNKT', 'CONJ', 'NOUN', 'VERB', 'ADJ', 'NOUN', 'CONJ', 'VERB', 'PREP', 'ADJ', 'ADJ', 'NOUN', 'PNKT', 'ADVB', 'PREP', 'NOUN', 'PRON', 'VERB', 'NOUN', 'PNKT', 'PRON', 'VERB', 'VERB', 'PRON', 'ADVB', 'PNKT', 'CONJ', 'PREP', 'ADJ', 'NOUN', 'NOUN', 'NOUN', 'VERB', 'CONJ', 'VERB', 'PREP', 'NOUN', 'VERB', 'ADVB', 'ADJ', 'PNKT', 'PNKT', 'CONJ', 'PRCL', 'VERB', 'PREP', 'NOUN', 'NOUN', 'PNKT', 'CONJ', 'VERB', 'ADJ', 'VERB', 'PREP', 'ADJ', 'NOUN', 'CONJ', 'CONJ', 'NOUN', 'PRCL', 'VERB', 'PNKT', 'PNKT', 'VERB', 'PRON', 'NOUN', 'PNKT']\n"
     ]
    }
   ],
   "source": [
    "print(golden_rus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Мария': 'NOUN'},\n",
      " {'затопила': 'VERB'},\n",
      " {'печь': 'NOUN'},\n",
      " {'и': 'CONJ'},\n",
      " {'принялась': 'VERB'},\n",
      " {'замешивать': 'VERB'},\n",
      " {'тесто': 'NOUN'},\n",
      " {'.': 'PNKT'},\n",
      " {'Ей': 'PRON'},\n",
      " {'всегда': 'ADVB'}]\n"
     ]
    }
   ],
   "source": [
    "# привести к читабельному виду\n",
    "tokened = word_tokenize(test_rus)\n",
    "pretty_golden_rus = []\n",
    "for w, g in zip(tokened, golden_rus):\n",
    "    pretty_golden_rus.append({w:g})\n",
    "pprint(pretty_golden_rus[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State your name and date of birth for the record, please.\n",
      "We need you to record more messages because your voice sounds more natural than mine.\n",
      "They must drive fast to get to that magic well in time.\n",
      "Several natural molecules perform a fast internal conversion.\n",
      "Somebody told me he works at that mine.\n",
      "Self-defense is also a constitutional right and fighting for it is the right thing to do.\n",
      "I do not feel well now so I need to lie down and rest for a bit.\n",
      "We are able to detect any lie you say, dear.\n",
      "All I did was light a match and then everything exploded!\n",
      "She picked my suit to match her wedding dress.\n"
     ]
    }
   ],
   "source": [
    "print(test_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State\n",
      "VERB\n",
      "your\n",
      "PRON\n",
      "name\n",
      "NOUN\n",
      "and\n",
      "CONJ\n",
      "date\n",
      "NOUN\n",
      "of\n",
      "PREP\n",
      "birth\n",
      "NOUN\n",
      "for\n",
      "PREP\n",
      "the\n",
      "DET\n",
      "record\n",
      "NOUN\n",
      ",\n",
      "PUNCT\n",
      "please\n",
      "INTJ\n",
      ".\n",
      "PUNCT\n",
      "We\n",
      "PRON\n",
      "need\n",
      "VERB\n",
      "you\n",
      "PRON\n",
      "to\n",
      "PRCL\n",
      "record\n",
      "VERB\n",
      "more\n",
      "ADJ\n",
      "messages\n",
      "NOUN\n",
      "because\n",
      "CONJ\n",
      "your\n",
      "PRON\n",
      "voice\n",
      "NOUN\n",
      "sounds\n",
      "VERB\n",
      "more\n",
      "ADJ\n",
      "natural\n",
      "ADJ\n",
      "than\n",
      "CONJ\n",
      "mine\n",
      "PRON\n",
      ".\n",
      "PUNCT\n",
      "They\n",
      "PRON\n",
      "must\n",
      "VERB\n",
      "drive\n",
      "VERB\n",
      "fast\n",
      "ADV\n",
      "to\n",
      "PRCL\n",
      "get\n",
      "VERB\n",
      "to\n",
      "PREP\n",
      "that\n",
      "DET\n",
      "magic\n",
      "ADJ\n",
      "well\n",
      "NOUN\n",
      "in\n",
      "PREP\n",
      "time\n",
      "NOUN\n",
      ".\n",
      "PUNCT\n",
      "Several\n",
      "ADJ\n",
      "natural\n",
      "ADJ\n",
      "molecules\n",
      "NOUN\n",
      "perform\n",
      "VERB\n",
      "a\n",
      "DET\n",
      "fast\n",
      "ADJ\n",
      "internal\n",
      "ADJ\n",
      "conversion\n",
      "NOUN\n",
      ".\n",
      "PUNCT\n",
      "Somebody\n",
      "PRON\n",
      "told\n",
      "VERB\n",
      "me\n",
      "PRON\n",
      "he\n",
      "PRON\n",
      "works\n",
      "VERB\n",
      "at\n",
      "PREP\n",
      "that\n",
      "DET\n",
      "mine\n",
      "NOUN\n",
      ".\n",
      "PUNCT\n",
      "Self-defense\n",
      "NOUN\n",
      "is\n",
      "VERB\n",
      "also\n",
      "ADV\n",
      "a\n",
      "DET\n",
      "constitutional\n",
      "ADJ\n",
      "right\n",
      "NOUN\n",
      "and\n",
      "CONJ\n",
      "fighting\n",
      "VERB\n",
      "for\n",
      "PREP\n",
      "it\n",
      "PRON\n",
      "is\n",
      "VERB\n",
      "the\n",
      "DET\n",
      "right\n",
      "ADJ\n",
      "thing\n",
      "NOUN\n",
      "to\n",
      "PRCL\n",
      "do\n",
      "VERB\n",
      ".\n",
      "PUNCT\n",
      "I\n",
      "PRON\n",
      "do\n",
      "VERB\n",
      "not\n",
      "PRCL\n",
      "feel\n",
      "VERB\n",
      "well\n",
      "ADV\n",
      "now\n",
      "ADV\n",
      "so\n",
      "CONJ\n",
      "I\n",
      "PRON\n",
      "need\n",
      "VERB\n",
      "to\n",
      "PRCL\n",
      "lie\n",
      "VERB\n",
      "down\n",
      "ADV\n",
      "and\n",
      "CONJ\n",
      "rest\n",
      "VERB\n",
      "for\n",
      "PREP\n",
      "a\n",
      "DET\n",
      "bit\n",
      "NOUN\n",
      ".\n",
      "PUNCT\n",
      "We\n",
      "PRON\n",
      "are\n",
      "VERB\n",
      "able\n",
      "ADJ\n",
      "to\n",
      "PRCL\n",
      "detect\n",
      "VERB\n",
      "any\n",
      "DET\n",
      "lie\n",
      "NOUN\n",
      "you\n",
      "PRON\n",
      "say\n",
      "VERB\n",
      ",\n",
      "PUNCT\n",
      "dear\n",
      "ADJ\n",
      ".\n",
      "PUNCT\n",
      "All\n",
      "PRON\n",
      "I\n",
      "PRON\n",
      "did\n",
      "VERB\n",
      "was\n",
      "VERB\n",
      "light\n",
      "VERB\n",
      "a\n",
      "DET\n",
      "match\n",
      "NOUN\n",
      "and\n",
      "CONJ\n",
      "then\n",
      "ADV\n",
      "everything\n",
      "PRON\n",
      "exploded\n",
      "VERB\n",
      "!\n",
      "PUNCT\n",
      "She\n",
      "PRON\n",
      "picked\n",
      "VERB\n",
      "my\n",
      "PRON\n",
      "suit\n",
      "NOUN\n",
      "to\n",
      "PRCL\n",
      "match\n",
      "VERB\n",
      "her\n",
      "PRON\n",
      "wedding\n",
      "NOUN\n",
      "dress\n",
      "NOUN\n",
      ".\n",
      "PUNCT\n"
     ]
    }
   ],
   "source": [
    "# разметить английский текст\n",
    "golden_eng = []\n",
    "for word in word_tokenize(test_eng):\n",
    "    print(word)\n",
    "    golden_eng.append(input())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['VERB', 'PRON', 'NOUN', 'CONJ', 'NOUN', 'PREP', 'NOUN', 'PREP', 'DET', 'NOUN', 'PUNCT', 'INTJ', 'PUNCT', 'PRON', 'VERB', 'PRON', 'PRCL', 'VERB', 'ADJ', 'NOUN', 'CONJ', 'PRON', 'NOUN', 'VERB', 'ADV', 'ADJ', 'CONJ', 'PRON', 'PUNCT', 'PRON', 'VERB', 'VERB', 'ADV', 'PRCL', 'VERB', 'PREP', 'DET', 'ADJ', 'NOUN', 'PREP', 'NOUN', 'PUNCT', 'ADJ', 'ADJ', 'NOUN', 'VERB', 'DET', 'ADJ', 'ADJ', 'NOUN', 'PUNCT', 'PRON', 'VERB', 'PRON', 'PRON', 'VERB', 'PREP', 'DET', 'NOUN', 'PUNCT', 'NOUN', 'VERB', 'ADV', 'DET', 'ADJ', 'NOUN', 'CONJ', 'VERB', 'PREP', 'PRON', 'VERB', 'DET', 'ADJ', 'NOUN', 'PRCL', 'VERB', 'PUNCT', 'PRON', 'VERB', 'PRCL', 'VERB', 'ADV', 'ADV', 'CONJ', 'PRON', 'VERB', 'PRCL', 'VERB', 'ADV', 'CONJ', 'VERB', 'PREP', 'DET', 'NOUN', 'PUNCT', 'PRON', 'VERB', 'ADJ', 'PRCL', 'VERB', 'DET', 'NOUN', 'PRON', 'VERB', 'PUNCT', 'ADJ', 'PUNCT', 'PRON', 'PRON', 'VERB', 'VERB', 'VERB', 'DET', 'NOUN', 'CONJ', 'ADV', 'PRON', 'VERB', 'PUNCT', 'PRON', 'VERB', 'PRON', 'NOUN', 'PRCL', 'VERB', 'PRON', 'NOUN', 'NOUN', 'PUNCT']\n"
     ]
    }
   ],
   "source": [
    "print(golden_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'State': 'VERB'},\n",
      " {'your': 'PRON'},\n",
      " {'name': 'NOUN'},\n",
      " {'and': 'CONJ'},\n",
      " {'date': 'NOUN'},\n",
      " {'of': 'PREP'},\n",
      " {'birth': 'NOUN'},\n",
      " {'for': 'PREP'},\n",
      " {'the': 'DET'},\n",
      " {'record': 'NOUN'}]\n"
     ]
    }
   ],
   "source": [
    "tokened = word_tokenize(test_eng)\n",
    "pretty_golden_eng = []\n",
    "for w, g in zip(tokened, golden_eng):\n",
    "    pretty_golden_eng.append({w:g})\n",
    "pprint(pretty_golden_eng[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Русские парсеры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# возвращает список тегов и читабельные словарики\n",
    "def test_pym2(text):\n",
    "    tags = []\n",
    "    pretty = []\n",
    "    for word in word_tokenize(text):\n",
    "        result = m.parse(word)[0].tag.POS\n",
    "        if result == None: # у пунктуации нет атрибута POS\n",
    "            tags.append('PNKT') # это для accuracy\n",
    "            pretty.append({word:'PNKT'}) # это посмотреть глазами\n",
    "        else:\n",
    "            tags.append(str(result))\n",
    "            pretty.append({word:str(result)})\n",
    "    return(tags, pretty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# то же самое для майстема\n",
    "def test_mystem(text):\n",
    "    tags = []\n",
    "    pretty = []\n",
    "    result = mst.analyze(text)\n",
    "    for res in result:\n",
    "        if 'analysis' in res:\n",
    "            gr = res['analysis'][0]['gr']\n",
    "            pos = gr.split('=')[0].split(',')[0]\n",
    "            tags.append(pos)\n",
    "            pretty.append({res['text']:pos})\n",
    "    return(tags, pretty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# то же самое для наташи\n",
    "def test_natasha(text):\n",
    "    tags = []\n",
    "    pretty = []\n",
    "    doc = Doc(text)\n",
    "    doc.segment(segmenter)\n",
    "    doc.tag_morph(morph_tagger)\n",
    "    for token in doc.tokens:\n",
    "        pretty.append({token.text: token.pos})\n",
    "        tags.append(token.pos)\n",
    "    return(tags, pretty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Мария': 'NOUN'},\n",
      " {'затопила': 'VERB'},\n",
      " {'печь': 'INFN'},\n",
      " {'и': 'CONJ'},\n",
      " {'принялась': 'VERB'},\n",
      " {'замешивать': 'INFN'},\n",
      " {'тесто': 'NOUN'},\n",
      " {'.': 'PNKT'},\n",
      " {'Ей': 'NPRO'},\n",
      " {'всегда': 'ADVB'}]\n"
     ]
    }
   ],
   "source": [
    "tags_pym2 = test_pym2(test_rus)\n",
    "pprint(tags_pym2[1][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Мария': 'S'},\n",
      " {'затопила': 'V'},\n",
      " {'печь': 'S'},\n",
      " {'и': 'CONJ'},\n",
      " {'принялась': 'V'},\n",
      " {'замешивать': 'V'},\n",
      " {'тесто': 'S'},\n",
      " {'Ей': 'SPRO'},\n",
      " {'всегда': 'ADVPRO'},\n",
      " {'нравилось': 'V'}]\n"
     ]
    }
   ],
   "source": [
    "tags_mystem = test_mystem(test_rus)\n",
    "pprint(tags_mystem[1][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Мария': 'PROPN'},\n",
      " {'затопила': 'VERB'},\n",
      " {'печь': 'NOUN'},\n",
      " {'и': 'CCONJ'},\n",
      " {'принялась': 'VERB'},\n",
      " {'замешивать': 'ADJ'},\n",
      " {'тесто': 'NOUN'},\n",
      " {'.': 'PUNCT'},\n",
      " {'Ей': 'PRON'},\n",
      " {'всегда': 'ADV'}]\n"
     ]
    }
   ],
   "source": [
    "tags_natasha = test_natasha(test_rus)\n",
    "pprint(tags_natasha[1][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проблема: у всех трёх парсеров разные теги (у наташи это формат UD) + майстем не парсит пунктуацию.  \n",
    "Решение:  \n",
    "1) убрать теги пунктуации везде, включая золотой стандарт;  \n",
    "2) привести теги к единому стандарту с помощью правил"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Унификация тегов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "pym2_rule = {\n",
    "    'ADJF':'ADJ',\n",
    "    'ADJS':'ADJ',\n",
    "    'COMP':'ADJ',\n",
    "    'ADVB':'ADV',\n",
    "    'NPRO':'PRON',\n",
    "    'INFN':'VERB',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "myst_rule = {\n",
    "    'S':'NOUN',\n",
    "    'A':'ADJ',\n",
    "    'V':'VERB',\n",
    "    'ADVPRO':'ADV',\n",
    "    'PR':'PREP',\n",
    "    'PART':'PRCL',\n",
    "    'APRO':'PRON',\n",
    "    'SPRO':'PRON',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "natasha_rule = {\n",
    "    'PROPN':'NOUN',\n",
    "    'AUX':'VERB',\n",
    "    'ADP':'PREP',\n",
    "    'CCONJ':'CONJ',\n",
    "    'SCONJ':'CONJ',\n",
    "    'PART':'PRCL',\n",
    "    'PUNCT':'PNKT',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unify(tagset, rule):\n",
    "    unified = []\n",
    "    for item in tagset:\n",
    "        if item in rule:\n",
    "            unified.append(rule[item])\n",
    "        else:\n",
    "            unified.append(item)\n",
    "    return unified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "unif_pym2 = unify(tags_pym2[0], pym2_rule)\n",
    "unif_myst = unify(tags_mystem[0], myst_rule)\n",
    "unif_natasha = unify(tags_natasha[0], natasha_rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# удалить тег пунктуации из всех списков:\n",
    "def del_pnkt(tag_list):\n",
    "    while 'PNKT' in tag_list:\n",
    "        tag_list.remove('PNKT')\n",
    "    return tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "punkted = [golden_rus, unif_pym2, unif_natasha]\n",
    "for p in punkted:\n",
    "    del_pnkt(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PRON', 'CONJ', 'PRCL', 'INTJ', 'VERB', 'NOUN', 'PREP', 'ADV', 'ADJ'}\n",
      "{'PRON', 'CONJ', 'PRCL', 'INTJ', 'VERB', 'NOUN', 'PREP', 'ADV', 'ADJ'}\n",
      "{'PRON', 'CONJ', 'PRCL', 'VERB', 'NOUN', 'DET', 'PREP', 'NUM', 'ADV', 'ADJ'}\n",
      "{'PRON', 'CONJ', 'PRCL', 'INTJ', 'ADVB', 'VERB', 'NOUN', 'PREP', 'ADJ'}\n"
     ]
    }
   ],
   "source": [
    "# сверить теги\n",
    "print(set(unif_pym2))\n",
    "print(set(unif_myst))\n",
    "print(set(unif_natasha))\n",
    "print(set(golden_rus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy pymorphy2: 0.7731\n",
      "Accuracy mystem: 0.8235\n",
      "Accuracy natasha: 0.7983\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy pymorphy2: %.4f\" % accuracy_score(unif_pym2, golden_rus))\n",
    "print(\"Accuracy mystem: %.4f\" % accuracy_score(unif_myst, golden_rus))\n",
    "print(\"Accuracy natasha: %.4f\" % accuracy_score(unif_natasha, golden_rus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Английские парсеры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State your name and date of birth for the record, please.\n",
      "We need you to record more messages because your voice sounds more natural than mine.\n",
      "They must drive fast to get to that magic well in time.\n",
      "Several natural molecules perform a fast internal conversion.\n",
      "Somebody told me he works at that mine.\n",
      "Self-defense is also a constitutional right and fighting for it is the right thing to do.\n",
      "I do not feel well now so I need to lie down and rest for a bit.\n",
      "We are able to detect any lie you say, dear.\n",
      "All I did was light a match and then everything exploded!\n",
      "She picked my suit to match her wedding dress.\n"
     ]
    }
   ],
   "source": [
    "print(test_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_nltk(text):\n",
    "    tags = []\n",
    "    pretty = []\n",
    "    for result in pos_tag(word_tokenize(text), tagset='universal'):\n",
    "        tags.append(result[1])\n",
    "        pretty.append({result[0]:result[1]})\n",
    "    return(tags, pretty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_spacy(text):\n",
    "    tags = []\n",
    "    pretty = []\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    for t in doc:\n",
    "        tags.append(t.pos_)\n",
    "        pretty.append({t.text:t.pos_})\n",
    "    return(tags, pretty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_flair(text):\n",
    "    tags = []\n",
    "    pretty = []\n",
    "    sentence = Sentence(test_eng)\n",
    "    tagger = SequenceTagger.load('upos')\n",
    "    tagger.predict(sentence)\n",
    "    for entity in sentence.to_dict(tag_type='pos')['entities']:\n",
    "        pos = re.match('[A-Z]+', str(entity['labels'][0])).group(0)\n",
    "        word = entity['text']\n",
    "        tags.append(pos)\n",
    "        pretty.append({word:pos})\n",
    "    return(tags, pretty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'State': 'VERB'},\n",
      " {'your': 'PRON'},\n",
      " {'name': 'NOUN'},\n",
      " {'and': 'CONJ'},\n",
      " {'date': 'NOUN'},\n",
      " {'of': 'ADP'},\n",
      " {'birth': 'NOUN'},\n",
      " {'for': 'ADP'},\n",
      " {'the': 'DET'},\n",
      " {'record': 'NOUN'}]\n"
     ]
    }
   ],
   "source": [
    "tags_nltk = test_nltk(test_eng)\n",
    "pprint(tags_nltk[1][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'State': 'VERB'},\n",
      " {'your': 'PRON'},\n",
      " {'name': 'NOUN'},\n",
      " {'and': 'CCONJ'},\n",
      " {'date': 'NOUN'},\n",
      " {'of': 'ADP'},\n",
      " {'birth': 'NOUN'},\n",
      " {'for': 'ADP'},\n",
      " {'the': 'DET'},\n",
      " {'record': 'NOUN'}]\n"
     ]
    }
   ],
   "source": [
    "tags_spacy = test_spacy(test_eng)\n",
    "pprint(tags_spacy[1][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-12 15:25:46,654 loading file C:\\Users\\User\\.flair\\models\\en-pos-ontonotes-v0.4.pt\n",
      "[{'State': 'NOUN'},\n",
      " {'your': 'PRON'},\n",
      " {'name': 'NOUN'},\n",
      " {'and': 'CCONJ'},\n",
      " {'date': 'NOUN'},\n",
      " {'of': 'ADP'},\n",
      " {'birth': 'NOUN'},\n",
      " {'for': 'ADP'},\n",
      " {'the': 'DET'},\n",
      " {'record': 'NOUN'}]\n"
     ]
    }
   ],
   "source": [
    "tags_flair = test_flair(test_eng)\n",
    "pprint(tags_flair[1][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Унификация тегов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Self': 'NOUN'}\n",
      "{'-': 'PUNCT'}\n",
      "{'defense': 'NOUN'}\n"
     ]
    }
   ],
   "source": [
    "# удалить вторую часть сложного слова,\n",
    "# чтобы количество ответов парсеров совпадало\n",
    "pprint(tags_spacy[1][65])\n",
    "pprint(tags_spacy[1][66])\n",
    "pprint(tags_spacy[1][67])\n",
    "del tags_spacy[0][67]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_rule = {\n",
    "    '.':'PUNCT',\n",
    "    'PRT':'PRCL',\n",
    "    'ADP':'PREP',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_rule = {\n",
    "    'AUX':'VERB',\n",
    "    'ADP':'PREP',\n",
    "    'CCONJ':'CONJ',\n",
    "    'SCONJ':'CONJ',\n",
    "    'PART':'PRCL',\n",
    "    'SPACE':'PUNCT',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "flair_rule = {\n",
    "    'AUX':'VERB',\n",
    "    'ADP':'PREP',\n",
    "    'CCONJ':'CONJ',\n",
    "    'SCONJ':'CONJ',\n",
    "    'PART':'PRCL',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "unif_nltk = unify(tags_nltk[0], nltk_rule)\n",
    "unif_spacy = unify(tags_spacy[0], spacy_rule)\n",
    "unif_flair = unify(tags_flair[0], flair_rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_punct(tag_list):\n",
    "    while 'PUNCT' in tag_list:\n",
    "        tag_list.remove('PUNCT')\n",
    "    return tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "puncted = [golden_eng, unif_nltk, unif_spacy, unif_flair]\n",
    "for p in puncted:\n",
    "    del_punct(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NOUN', 'VERB', 'ADJ', 'PRCL', 'DET', 'PREP', 'ADV', 'CONJ', 'PRON'}\n",
      "{'NOUN', 'VERB', 'INTJ', 'ADJ', 'PRCL', 'DET', 'PREP', 'ADV', 'CONJ', 'PRON'}\n",
      "{'NOUN', 'INTJ', 'VERB', 'ADJ', 'PRCL', 'DET', 'PREP', 'ADV', 'CONJ', 'PRON'}\n",
      "{'NOUN', 'VERB', 'INTJ', 'ADJ', 'PRCL', 'DET', 'PREP', 'ADV', 'CONJ', 'PRON'}\n"
     ]
    }
   ],
   "source": [
    "print(set(unif_nltk))\n",
    "print(set(unif_spacy))\n",
    "print(set(unif_flair))\n",
    "print(set(golden_eng))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy nltk: 0.8718\n",
      "Accuracy spacy: 0.9487\n",
      "Accuracy flair: 0.8974\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy nltk: %.4f\" % accuracy_score(unif_nltk, golden_eng))\n",
    "print(\"Accuracy spacy: %.4f\" % accuracy_score(unif_spacy, golden_eng))\n",
    "print(\"Accuracy flair: %.4f\" % accuracy_score(unif_flair, golden_eng))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Итог\n",
    "Итак, среди русскоязычных парсеров побеждает Mystem, а среди англоязычных - SpaCy. Но ни один из них не смог стопроцентно верно разметить мои тексты."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
