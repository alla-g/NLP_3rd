{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "import spacy\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pprint import pprint\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>target</th>\n",
       "      <th>target_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: twillis@ec.ecn.purdue.edu (Thomas E Will...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: jgreen@amber (Joe Green)\\nSubject: Re: W...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: jcm@head-cfa.harvard.edu (Jonathan McDow...</td>\n",
       "      <td>14</td>\n",
       "      <td>sci.space</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  target  \\\n",
       "0  From: lerxst@wam.umd.edu (where's my thing)\\nS...       7   \n",
       "1  From: guykuo@carson.u.washington.edu (Guy Kuo)...       4   \n",
       "2  From: twillis@ec.ecn.purdue.edu (Thomas E Will...       4   \n",
       "3  From: jgreen@amber (Joe Green)\\nSubject: Re: W...       1   \n",
       "4  From: jcm@head-cfa.harvard.edu (Jonathan McDow...      14   \n",
       "\n",
       "            target_names  \n",
       "0              rec.autos  \n",
       "1  comp.sys.mac.hardware  \n",
       "2  comp.sys.mac.hardware  \n",
       "3          comp.graphics  \n",
       "4              sci.space  "
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw = pd.read_json('https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json')\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['From: (wheres my thing) Subject: WHAT car is this!? Nntp-Posting-Host: '\n",
      " 'rac3.wam.umd.edu Organization: University of Maryland, College Park Lines: '\n",
      " '15 I was wondering if anyone out there could enlighten me on this car I saw '\n",
      " 'the other day. It was a 2-door sports car, looked to be from the late 60s/ '\n",
      " 'early 70s. It was called a Bricklin. The doors were really small. In '\n",
      " 'addition, the front bumper was separate from the rest of the body. This is '\n",
      " 'all I know. If anyone can tellme a model name, engine specs, years of '\n",
      " 'production, where this car is made, history, or whatever info you have on '\n",
      " 'this funky looking car, please e-mail. Thanks, - IL ---- brought to you by '\n",
      " 'your neighborhood Lerxst ---- ']\n"
     ]
    }
   ],
   "source": [
    "# Convert to list\n",
    "data = df_raw.content.values.tolist()\n",
    "\n",
    "# Remove Emails\n",
    "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "\n",
    "# Remove new line characters\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "pprint(data[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['from', 'wheres', 'my', 'thing', 'subject', 'what', 'car', 'is', 'this', 'nntp', 'posting', 'host', 'rac', 'wam', 'umd', 'edu', 'organization', 'university', 'of', 'maryland', 'college', 'park', 'lines', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'saw', 'the', 'other', 'day', 'it', 'was', 'door', 'sports', 'car', 'looked', 'to', 'be', 'from', 'the', 'late', 'early', 'it', 'was', 'called', 'bricklin', 'the', 'doors', 'were', 'really', 'small', 'in', 'addition', 'the', 'front', 'bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body', 'this', 'is', 'all', 'know', 'if', 'anyone', 'can', 'tellme', 'model', 'name', 'engine', 'specs', 'years', 'of', 'production', 'where', 'this', 'car', 'is', 'made', 'history', 'or', 'whatever', 'info', 'you', 'have', 'on', 'this', 'funky', 'looking', 'car', 'please', 'mail', 'thanks', 'il', 'brought', 'to', 'you', 'by', 'your', 'neighborhood', 'lerxst']]\n"
     ]
    }
   ],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['from', 'wheres', 'my', 'thing', 'subject', 'what', 'car', 'is', 'this', 'nntp_posting_host', 'rac_wam_umd_edu', 'organization', 'university', 'of', 'maryland_college_park', 'lines', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'saw', 'the', 'other', 'day', 'it', 'was', 'door', 'sports', 'car', 'looked', 'to', 'be', 'from', 'the', 'late', 'early', 'it', 'was', 'called', 'bricklin', 'the', 'doors', 'were', 'really', 'small', 'in', 'addition', 'the', 'front_bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body', 'this', 'is', 'all', 'know', 'if', 'anyone', 'can', 'tellme', 'model', 'name', 'engine', 'specs', 'years', 'of', 'production', 'where', 'this', 'car', 'is', 'made', 'history', 'or', 'whatever', 'info', 'you', 'have', 'on', 'this', 'funky', 'looking', 'car', 'please', 'mail', 'thanks', 'il', 'brought', 'to', 'you', 'by', 'your', 'neighborhood', 'lerxst']\n"
     ]
    }
   ],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['where', 'thing', 'car', 'nntp_poste', 'host', 'park', 'line', 'wonder', 'could', 'enlighten', 'car', 'see', 'day', 'door', 'sport', 'car', 'look', 'late', 'early', 'call', 'bricklin', 'door', 'really', 'small', 'addition', 'separate', 'rest', 'body', 'know', 'tellme', 'model', 'name', 'engine', 'year', 'production', 'car', 'make', 'history', 'info', 'funky', 'look', 'car', 'mail', 'thank', 'bring', 'neighborhood', 'lerxst']]\n"
     ]
    }
   ],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 5), (6, 1), (7, 1), (8, 2), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 2), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('addition', 1),\n",
       "  ('body', 1),\n",
       "  ('bricklin', 1),\n",
       "  ('bring', 1),\n",
       "  ('call', 1),\n",
       "  ('car', 5),\n",
       "  ('could', 1),\n",
       "  ('day', 1),\n",
       "  ('door', 2),\n",
       "  ('early', 1),\n",
       "  ('engine', 1),\n",
       "  ('enlighten', 1),\n",
       "  ('funky', 1),\n",
       "  ('history', 1),\n",
       "  ('host', 1),\n",
       "  ('info', 1),\n",
       "  ('know', 1),\n",
       "  ('late', 1),\n",
       "  ('lerxst', 1),\n",
       "  ('line', 1),\n",
       "  ('look', 2),\n",
       "  ('mail', 1),\n",
       "  ('make', 1),\n",
       "  ('model', 1),\n",
       "  ('name', 1),\n",
       "  ('neighborhood', 1),\n",
       "  ('nntp_poste', 1),\n",
       "  ('park', 1),\n",
       "  ('production', 1),\n",
       "  ('really', 1),\n",
       "  ('rest', 1),\n",
       "  ('see', 1),\n",
       "  ('separate', 1),\n",
       "  ('small', 1),\n",
       "  ('sport', 1),\n",
       "  ('tellme', 1),\n",
       "  ('thank', 1),\n",
       "  ('thing', 1),\n",
       "  ('where', 1),\n",
       "  ('wonder', 1),\n",
       "  ('year', 1)]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Прогон на базовом дженсиме:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.097*\"black\" + 0.073*\"wing\" + 0.072*\"white\" + 0.048*\"vote\" + '\n",
      "  '0.041*\"flight\" + 0.028*\"devil\" + 0.027*\"capacity\" + 0.024*\"trace\" + '\n",
      "  '0.023*\"assist\" + 0.022*\"penalty\"'),\n",
      " (1,\n",
      "  '0.064*\"man\" + 0.040*\"god\" + 0.039*\"accept\" + 0.034*\"explain\" + '\n",
      "  '0.030*\"member\" + 0.029*\"age\" + 0.027*\"israeli\" + 0.027*\"season\" + '\n",
      "  '0.024*\"publish\" + 0.021*\"serious\"'),\n",
      " (2,\n",
      "  '0.130*\"board\" + 0.054*\"expansion\" + 0.032*\"looking\" + 0.016*\"stuff_delete\" '\n",
      "  '+ 0.011*\"rod\" + 0.000*\"reluctant\" + 0.000*\"wire\" + 0.000*\"connect\" + '\n",
      "  '0.000*\"worked\" + 0.000*\"blanking\"'),\n",
      " (3,\n",
      "  '0.060*\"reality\" + 0.053*\"picture\" + 0.050*\"object\" + 0.042*\"greek\" + '\n",
      "  '0.038*\"contain\" + 0.036*\"generate\" + 0.034*\"interface\" + 0.030*\"font\" + '\n",
      "  '0.029*\"concept\" + 0.026*\"workstation\"'),\n",
      " (4,\n",
      "  '0.215*\"ax\" + 0.182*\"max\" + 0.040*\"orbit\" + 0.035*\"satellite\" + '\n",
      "  '0.034*\"mission\" + 0.026*\"launch\" + 0.022*\"moon\" + 0.020*\"shuttle\" + '\n",
      "  '0.018*\"spacecraft\" + 0.017*\"lunar\"'),\n",
      " (5,\n",
      "  '0.032*\"kill\" + 0.024*\"child\" + 0.023*\"government\" + 0.022*\"people\" + '\n",
      "  '0.019*\"death\" + 0.018*\"attack\" + 0.017*\"country\" + 0.016*\"fire\" + '\n",
      "  '0.016*\"war\" + 0.015*\"woman\"'),\n",
      " (6,\n",
      "  '0.037*\"window\" + 0.034*\"card\" + 0.031*\"run\" + 0.028*\"problem\" + '\n",
      "  '0.027*\"computer\" + 0.026*\"bit\" + 0.025*\"thank\" + 0.024*\"driver\" + '\n",
      "  '0.024*\"price\" + 0.024*\"use\"'),\n",
      " (7,\n",
      "  '0.030*\"people\" + 0.023*\"say\" + 0.018*\"believe\" + 0.018*\"reason\" + '\n",
      "  '0.016*\"evidence\" + 0.014*\"may\" + 0.012*\"fact\" + 0.012*\"state\" + '\n",
      "  '0.011*\"mean\" + 0.011*\"claim\"'),\n",
      " (8,\n",
      "  '0.095*\"book\" + 0.068*\"physical\" + 0.062*\"science\" + 0.047*\"study\" + '\n",
      "  '0.032*\"author\" + 0.029*\"revelation\" + 0.027*\"avoid\" + 0.026*\"direct\" + '\n",
      "  '0.021*\"animal\" + 0.020*\"objective\"'),\n",
      " (9,\n",
      "  '0.062*\"program\" + 0.062*\"file\" + 0.059*\"mail\" + 0.045*\"send\" + '\n",
      "  '0.043*\"information\" + 0.035*\"list\" + 0.033*\"address\" + 0.031*\"email\" + '\n",
      "  '0.029*\"image\" + 0.028*\"copy\"'),\n",
      " (10,\n",
      "  '0.087*\"phone\" + 0.076*\"package\" + 0.052*\"nntp_posting\" + 0.051*\"route\" + '\n",
      "  '0.036*\"fax\" + 0.034*\"link\" + 0.032*\"btw\" + 0.032*\"primarily\" + '\n",
      "  '0.023*\"trick\" + 0.023*\"direction\"'),\n",
      " (11,\n",
      "  '0.182*\"drive\" + 0.119*\"car\" + 0.032*\"engine\" + 0.029*\"ride\" + 0.022*\"cop\" + '\n",
      "  '0.020*\"dealer\" + 0.019*\"mile\" + 0.018*\"buy\" + 0.017*\"road\" + '\n",
      "  '0.017*\"battery\"'),\n",
      " (12,\n",
      "  '0.085*\"player\" + 0.043*\"university\" + 0.033*\"switch\" + 0.032*\"cool\" + '\n",
      "  '0.029*\"printer\" + 0.028*\"baseball\" + 0.027*\"hot\" + 0.027*\"star\" + '\n",
      "  '0.020*\"blow\" + 0.020*\"helmet\"'),\n",
      " (13,\n",
      "  '0.098*\"software\" + 0.074*\"test\" + 0.051*\"pc\" + 0.044*\"product\" + '\n",
      "  '0.039*\"bus\" + 0.037*\"scsi\" + 0.031*\"internal\" + 0.030*\"device\" + '\n",
      "  '0.028*\"motif\" + 0.028*\"transfer\"'),\n",
      " (14,\n",
      "  '0.083*\"patient\" + 0.051*\"family\" + 0.045*\"box\" + 0.039*\"treatment\" + '\n",
      "  '0.038*\"disease\" + 0.036*\"doctor\" + 0.035*\"cd\" + 0.026*\"status\" + '\n",
      "  '0.025*\"medical\" + 0.024*\"wave\"'),\n",
      " (15,\n",
      "  '0.065*\"input\" + 0.053*\"eat\" + 0.049*\"material\" + 0.045*\"controller\" + '\n",
      "  '0.042*\"signal\" + 0.041*\"trust\" + 0.038*\"ground\" + 0.037*\"expensive\" + '\n",
      "  '0.031*\"output\" + 0.029*\"circuit\"'),\n",
      " (16,\n",
      "  '0.103*\"key\" + 0.057*\"gun\" + 0.055*\"chip\" + 0.047*\"public\" + '\n",
      "  '0.041*\"government\" + 0.029*\"encryption\" + 0.027*\"security\" + '\n",
      "  '0.025*\"private\" + 0.019*\"weapon\" + 0.018*\"secure\"'),\n",
      " (17,\n",
      "  '0.020*\"use\" + 0.018*\"also\" + 0.016*\"system\" + 0.014*\"may\" + 0.014*\"number\" '\n",
      "  '+ 0.013*\"new\" + 0.009*\"high\" + 0.008*\"work\" + 0.008*\"will\" + 0.008*\"need\"'),\n",
      " (18,\n",
      "  '0.078*\"team\" + 0.071*\"game\" + 0.055*\"year\" + 0.054*\"play\" + 0.050*\"win\" + '\n",
      "  '0.024*\"hit\" + 0.023*\"fan\" + 0.023*\"goal\" + 0.022*\"run\" + 0.022*\"score\"'),\n",
      " (19,\n",
      "  '0.039*\"would\" + 0.038*\"line\" + 0.033*\"write\" + 0.021*\"article\" + '\n",
      "  '0.020*\"know\" + 0.019*\"be\" + 0.018*\"go\" + 0.017*\"get\" + 0.015*\"think\" + '\n",
      "  '0.015*\"good\"')]\n"
     ]
    }
   ],
   "source": [
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Прогон через маллет:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "id": "zZejQsFDVZYn"
   },
   "outputs": [],
   "source": [
    "os.environ.update({'MALLET_HOME':r'C:/Users/User/Desktop/универ/автобрея/mallet-2.0.8/'})\n",
    "mallet_path = r'C:\\Users\\User\\Desktop\\универ\\автобрея\\mallet-2.0.8\\bin\\mallet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_mallet(mallet_path, corpus, num_topics, id2word, data_lemmatized, output):\n",
    "    ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "    coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "    coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
    "    if output == True:\n",
    "        pprint(ldamallet.show_topics(formatted=True, num_topics=20))\n",
    "        print('\\nCoherence Score: ', coherence_ldamallet)\n",
    "    return coherence_ldamallet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пробный прогон на 20-ти топиках:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.021*\"make\" + 0.017*\"work\" + 0.016*\"money\" + 0.016*\"year\" + 0.013*\"pay\" + '\n",
      "  '0.012*\"people\" + 0.012*\"job\" + 0.010*\"government\" + 0.008*\"cost\" + '\n",
      "  '0.008*\"time\"'),\n",
      " (1,\n",
      "  '0.014*\"people\" + 0.011*\"man\" + 0.011*\"word\" + 0.011*\"christian\" + '\n",
      "  '0.011*\"love\" + 0.010*\"church\" + 0.010*\"religion\" + 0.010*\"time\" + '\n",
      "  '0.009*\"book\" + 0.009*\"make\"'),\n",
      " (2,\n",
      "  '0.014*\"bike\" + 0.010*\"turn\" + 0.010*\"ride\" + 0.010*\"light\" + 0.009*\"ground\" '\n",
      "  '+ 0.008*\"back\" + 0.008*\"wire\" + 0.008*\"power\" + 0.008*\"side\" + '\n",
      "  '0.008*\"water\"'),\n",
      " (3,\n",
      "  '0.172*\"ax\" + 0.138*\"max\" + 0.057*\"car\" + 0.053*\"line\" + 0.031*\"buy\" + '\n",
      "  '0.029*\"price\" + 0.027*\"sell\" + 0.024*\"sale\" + 0.015*\"distribution_usa\" + '\n",
      "  '0.011*\"interested\"'),\n",
      " (4,\n",
      "  '0.019*\"people\" + 0.018*\"happen\" + 0.017*\"time\" + 0.014*\"leave\" + '\n",
      "  '0.013*\"back\" + 0.013*\"start\" + 0.012*\"day\" + 0.009*\"live\" + 0.009*\"woman\" + '\n",
      "  '0.009*\"home\"'),\n",
      " (5,\n",
      "  '0.034*\"game\" + 0.029*\"team\" + 0.029*\"year\" + 0.026*\"play\" + 0.020*\"player\" '\n",
      "  '+ 0.018*\"win\" + 0.014*\"good\" + 0.012*\"season\" + 0.011*\"hit\" + 0.010*\"lose\"'),\n",
      " (6,\n",
      "  '0.051*\"good\" + 0.038*\"thing\" + 0.035*\"write\" + 0.033*\"make\" + 0.023*\"bad\" + '\n",
      "  '0.013*\"opinion\" + 0.013*\"guess\" + 0.013*\"lot\" + 0.013*\"post\" + '\n",
      "  '0.013*\"pretty\"'),\n",
      " (7,\n",
      "  '0.027*\"gun\" + 0.018*\"state\" + 0.013*\"law\" + 0.012*\"people\" + 0.011*\"crime\" '\n",
      "  '+ 0.011*\"case\" + 0.010*\"fire\" + 0.010*\"weapon\" + 0.009*\"kill\" + '\n",
      "  '0.009*\"police\"'),\n",
      " (8,\n",
      "  '0.026*\"window\" + 0.018*\"image\" + 0.017*\"program\" + 0.017*\"version\" + '\n",
      "  '0.015*\"software\" + 0.014*\"file\" + 0.014*\"application\" + 0.014*\"display\" + '\n",
      "  '0.013*\"server\" + 0.013*\"run\"'),\n",
      " (9,\n",
      "  '0.012*\"problem\" + 0.012*\"drug\" + 0.011*\"study\" + 0.009*\"food\" + '\n",
      "  '0.008*\"doctor\" + 0.008*\"effect\" + 0.007*\"patient\" + 0.007*\"eat\" + '\n",
      "  '0.007*\"time\" + 0.007*\"day\"'),\n",
      " (10,\n",
      "  '0.039*\"key\" + 0.018*\"system\" + 0.014*\"encryption\" + 0.012*\"chip\" + '\n",
      "  '0.011*\"technology\" + 0.011*\"government\" + 0.010*\"public\" + 0.010*\"security\" '\n",
      "  '+ 0.010*\"phone\" + 0.010*\"bit\"'),\n",
      " (11,\n",
      "  '0.044*\"file\" + 0.029*\"line\" + 0.019*\"change\" + 0.018*\"read\" + '\n",
      "  '0.017*\"program\" + 0.016*\"set\" + 0.016*\"follow\" + 0.016*\"number\" + '\n",
      "  '0.011*\"entry\" + 0.011*\"return\"'),\n",
      " (12,\n",
      "  '0.029*\"question\" + 0.018*\"claim\" + 0.018*\"exist\" + 0.017*\"true\" + '\n",
      "  '0.015*\"reason\" + 0.014*\"argument\" + 0.013*\"evidence\" + 0.013*\"answer\" + '\n",
      "  '0.012*\"make\" + 0.010*\"truth\"'),\n",
      " (13,\n",
      "  '0.022*\"space\" + 0.012*\"system\" + 0.010*\"launch\" + 0.009*\"project\" + '\n",
      "  '0.008*\"design\" + 0.008*\"technology\" + 0.008*\"base\" + 0.007*\"year\" + '\n",
      "  '0.007*\"satellite\" + 0.006*\"mission\"'),\n",
      " (14,\n",
      "  '0.032*\"people\" + 0.017*\"law\" + 0.011*\"write\" + 0.011*\"opinion\" + '\n",
      "  '0.009*\"moral\" + 0.009*\"person\" + 0.009*\"point\" + 0.009*\"system\" + '\n",
      "  '0.008*\"man\" + 0.008*\"society\"'),\n",
      " (15,\n",
      "  '0.038*\"drive\" + 0.034*\"problem\" + 0.029*\"card\" + 0.024*\"system\" + '\n",
      "  '0.021*\"driver\" + 0.020*\"work\" + 0.015*\"disk\" + 0.014*\"scsi\" + '\n",
      "  '0.013*\"monitor\" + 0.013*\"machine\"'),\n",
      " (16,\n",
      "  '0.203*\"write\" + 0.189*\"line\" + 0.149*\"article\" + 0.091*\"host\" + '\n",
      "  '0.063*\"organization\" + 0.032*\"nntp_poste\" + 0.031*\"reply\" + 0.011*\"hear\" + '\n",
      "  '0.009*\"university\" + 0.008*\"nntp_posting\"'),\n",
      " (17,\n",
      "  '0.035*\"mail\" + 0.029*\"post\" + 0.027*\"send\" + 0.024*\"list\" + '\n",
      "  '0.021*\"information\" + 0.018*\"address\" + 0.016*\"email\" + 0.016*\"include\" + '\n",
      "  '0.016*\"internet\" + 0.015*\"group\"'),\n",
      " (18,\n",
      "  '0.016*\"people\" + 0.010*\"israeli\" + 0.010*\"attack\" + 0.010*\"war\" + '\n",
      "  '0.009*\"government\" + 0.009*\"kill\" + 0.009*\"turkish\" + 0.008*\"land\" + '\n",
      "  '0.008*\"armenian\" + 0.008*\"give\"'),\n",
      " (19,\n",
      "  '0.029*\"line\" + 0.028*\"time\" + 0.026*\"high\" + 0.016*\"bit\" + 0.016*\"speed\" + '\n",
      "  '0.014*\"good\" + 0.014*\"point\" + 0.014*\"fast\" + 0.013*\"low\" + '\n",
      "  '0.010*\"performance\"')]\n",
      "\n",
      "Coherence Score:  0.5449010381050419\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5449010381050419"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "do_mallet(mallet_path, corpus, 20, id2word, data_lemmatized, output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вычисление оптимального числа топиков  \n",
    "Функция создаёт модели в пределах задаваемого диапазона топиков, выбирает лучший Coherence score и возращает словарь всех результатов, а также отдельно оптимальное количество топиков и скор этого количества"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_number(min_topics, max_topics):\n",
    "    results = {}\n",
    "    for number in range(min_topics, max_topics+1):\n",
    "        score = do_mallet(mallet_path, corpus, number, id2word, data_lemmatized, output=False)\n",
    "        results[number] = score\n",
    "    best_number = max(results, key=results.get)\n",
    "    best_score = max(results.values())\n",
    "    return results, best_number, best_score    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# для диапазона 10-25 включительно:\n",
    "output = optimal_number(10,25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Все результаты:\n",
      "10 topics -- score 0.5148061073978827\n",
      "11 topics -- score 0.4992618599471466\n",
      "12 topics -- score 0.5156948040184587\n",
      "13 topics -- score 0.5282312848308736\n",
      "14 topics -- score 0.5115733675113443\n",
      "15 topics -- score 0.5316952226416727\n",
      "16 topics -- score 0.5377685072500002\n",
      "17 topics -- score 0.5414166468586689\n",
      "18 topics -- score 0.5396985998753948\n",
      "19 topics -- score 0.5418557983901511\n",
      "20 topics -- score 0.5406957559477352\n",
      "21 topics -- score 0.5472890517871054\n",
      "22 topics -- score 0.5397009196797569\n",
      "23 topics -- score 0.5297457794647642\n",
      "24 topics -- score 0.5280045323061477\n",
      "25 topics -- score 0.53705585712109\n",
      "Лучшее количество топиков: 21\n",
      "Скор этого количества: 0.5472890517871054\n"
     ]
    }
   ],
   "source": [
    "print('Все результаты:')\n",
    "for o in output[0]:\n",
    "    print(o, 'topics -- score', output[0][o])\n",
    "print('Лучшее количество топиков:', output[1])\n",
    "print('Скор этого количества:', output[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Определение главного топика в текстах:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=21, id2word=id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  [('power', 0.022116357055410655),\n",
      "   ('ground', 0.012029594932666206),\n",
      "   ('line', 0.011923138340341725),\n",
      "   ('current', 0.010911800713259168),\n",
      "   ('high', 0.01048597434396125),\n",
      "   ('wire', 0.009581093309203172),\n",
      "   ('water', 0.009128652791824133),\n",
      "   ('light', 0.008889125459094054),\n",
      "   ('unit', 0.008782668866769575),\n",
      "   ('low', 0.008223771757066057)]),\n",
      " (1,\n",
      "  [('bit', 0.029886417954612707),\n",
      "   ('card', 0.023877267657569482),\n",
      "   ('driver', 0.018482689549996587),\n",
      "   ('line', 0.016843830378075707),\n",
      "   ('file', 0.016547925249812214),\n",
      "   ('program', 0.012906015978876926),\n",
      "   ('memory', 0.012405253454123324),\n",
      "   ('mode', 0.011312680672842738),\n",
      "   ('speed', 0.011153347142239319),\n",
      "   ('build', 0.0109940136116359)]),\n",
      " (2,\n",
      "  [('file', 0.03245635148395697),\n",
      "   ('image', 0.01964414656273722),\n",
      "   ('include', 0.01834811427301846),\n",
      "   ('information', 0.017255744200255503),\n",
      "   ('list', 0.01457110588583807),\n",
      "   ('software', 0.013812001259002796),\n",
      "   ('mail', 0.013793486512006813),\n",
      "   ('send', 0.01312695562015145),\n",
      "   ('address', 0.011553202125492955),\n",
      "   ('internet', 0.011219936679565274)]),\n",
      " (3,\n",
      "  [('ax', 0.1935034041506029),\n",
      "   ('max', 0.15593470593060454),\n",
      "   ('drive', 0.07308670330571733),\n",
      "   ('scsi', 0.024813386924780575),\n",
      "   ('line', 0.02350094331884177),\n",
      "   ('disk', 0.021696333360675907),\n",
      "   ('system', 0.01980969567713887),\n",
      "   ('problem', 0.01591337872200804),\n",
      "   ('boot', 0.008366827987859897),\n",
      "   ('controller', 0.008202772537117545)]),\n",
      " (4,\n",
      "  [('make', 0.04285966382847311),\n",
      "   ('people', 0.03864649049278687),\n",
      "   ('thing', 0.03791856002117616),\n",
      "   ('post', 0.03416861516742401),\n",
      "   ('good', 0.027617240922927604),\n",
      "   ('question', 0.021440861163806415),\n",
      "   ('bad', 0.018110027793708915),\n",
      "   ('time', 0.015860060881457624),\n",
      "   ('give', 0.015551241893501567),\n",
      "   ('feel', 0.014602726430493668)]),\n",
      " (5,\n",
      "  [('word', 0.013654650310152797),\n",
      "   ('love', 0.013575378029687469),\n",
      "   ('people', 0.013238470837709824),\n",
      "   ('man', 0.011653025228403257),\n",
      "   ('life', 0.010701757862819319),\n",
      "   ('religion', 0.010701757862819319),\n",
      "   ('christian', 0.010642303652470323),\n",
      "   ('church', 0.01004776154898036),\n",
      "   ('day', 0.008601042430488119),\n",
      "   ('faith', 0.00794704611664916)]),\n",
      " (6,\n",
      "  [('report', 0.014313327298084216),\n",
      "   ('group', 0.013016564312103936),\n",
      "   ('drug', 0.011426194612316802),\n",
      "   ('year', 0.010936850089305375),\n",
      "   ('number', 0.010741112280100804),\n",
      "   ('show', 0.010423038340143378),\n",
      "   ('member', 0.00990922659098138),\n",
      "   ('case', 0.009860292138680237),\n",
      "   ('school', 0.009762423234077952),\n",
      "   ('people', 0.009444349294120526)]),\n",
      " (7,\n",
      "  [('find', 0.037096813635585556),\n",
      "   ('book', 0.027437458733768617),\n",
      "   ('point', 0.026116939329469593),\n",
      "   ('number', 0.02264446237742401),\n",
      "   ('time', 0.02183747829701905),\n",
      "   ('read', 0.0205414129557626),\n",
      "   ('give', 0.015577238158119972),\n",
      "   ('reference', 0.012936199349521922),\n",
      "   ('problem', 0.012055853079989241),\n",
      "   ('call', 0.011542317756095176)]),\n",
      " (8,\n",
      "  [('people', 0.01710040260656622),\n",
      "   ('attack', 0.010916033702735235),\n",
      "   ('kill', 0.010812269123811896),\n",
      "   ('israeli', 0.010750010376457892),\n",
      "   ('war', 0.010355704976549205),\n",
      "   ('turkish', 0.009006765450545802),\n",
      "   ('government', 0.00884074212426846),\n",
      "   ('land', 0.008467189640144441),\n",
      "   ('armenian', 0.008342672145436434),\n",
      "   ('today', 0.007139003029925705)]),\n",
      " (9,\n",
      "  [('gun', 0.0235866508805683),\n",
      "   ('law', 0.0202382714222288),\n",
      "   ('state', 0.018998816042622466),\n",
      "   ('government', 0.016260914607074146),\n",
      "   ('people', 0.015816930590498744),\n",
      "   ('make', 0.011950569779487939),\n",
      "   ('country', 0.008398697646884713),\n",
      "   ('weapon', 0.008195204972620985),\n",
      "   ('crime', 0.007566227615805831),\n",
      "   ('public', 0.007362734941542105)]),\n",
      " (10,\n",
      "  [('space', 0.020714689388340653),\n",
      "   ('work', 0.013585184541146806),\n",
      "   ('year', 0.012324479415728383),\n",
      "   ('system', 0.010715993566056601),\n",
      "   ('program', 0.010629048384993262),\n",
      "   ('project', 0.009477024735904012),\n",
      "   ('launch', 0.009346606964309003),\n",
      "   ('cost', 0.007803330000434726),\n",
      "   ('plan', 0.006825196713472156),\n",
      "   ('earth', 0.006499152284484632)]),\n",
      " (11,\n",
      "  [('bike', 0.01684602099109871),\n",
      "   ('time', 0.01575660953899296),\n",
      "   ('turn', 0.012833798326026305),\n",
      "   ('ride', 0.011824099907001461),\n",
      "   ('speed', 0.009884416102032683),\n",
      "   ('leave', 0.009645277002789956),\n",
      "   ('side', 0.00871529161684602),\n",
      "   ('back', 0.008635578583765112),\n",
      "   ('road', 0.00757273814268633),\n",
      "   ('fast', 0.0069084628670120895)]),\n",
      " (12,\n",
      "  [('game', 0.03402771836769612),\n",
      "   ('team', 0.02910856360680982),\n",
      "   ('year', 0.028873299683463086),\n",
      "   ('play', 0.02605013260330225),\n",
      "   ('player', 0.020125759260843527),\n",
      "   ('win', 0.017088715886731113),\n",
      "   ('good', 0.01595517152878775),\n",
      "   ('season', 0.01238343741979639),\n",
      "   ('fan', 0.009581657969030712),\n",
      "   ('score', 0.00951749508084524)]),\n",
      " (13,\n",
      "  [('line', 0.09198970992637275),\n",
      "   ('mail', 0.027972441526952305),\n",
      "   ('host', 0.027440196339335875),\n",
      "   ('sale', 0.01972264111889766),\n",
      "   ('computer', 0.01800762884768917),\n",
      "   ('price', 0.01765279872261155),\n",
      "   ('interested', 0.015996924805582662),\n",
      "   ('sell', 0.015021141961619208),\n",
      "   ('send', 0.013986220763476153),\n",
      "   ('sound', 0.013720098169667938)]),\n",
      " (14,\n",
      "  [('window', 0.04171144545943042),\n",
      "   ('problem', 0.024673114812824647),\n",
      "   ('run', 0.023217804048002867),\n",
      "   ('set', 0.020307182518359303),\n",
      "   ('display', 0.017262224610424502),\n",
      "   ('application', 0.016120365394948953),\n",
      "   ('work', 0.015560630485402113),\n",
      "   ('program', 0.013859036360379724),\n",
      "   ('screen', 0.013030628694250403),\n",
      "   ('server', 0.012918681712341035)]),\n",
      " (15,\n",
      "  [('key', 0.04125268989515132),\n",
      "   ('system', 0.015841765486928255),\n",
      "   ('encryption', 0.01524655464493384),\n",
      "   ('chip', 0.01307174579918502),\n",
      "   ('public', 0.01124032782381759),\n",
      "   ('government', 0.011194542374433404),\n",
      "   ('bit', 0.011125864200357126),\n",
      "   ('message', 0.011034293301588755),\n",
      "   ('technology', 0.01091982967812829),\n",
      "   ('security', 0.010690902431207362)]),\n",
      " (16,\n",
      "  [('exist', 0.01622839969947408),\n",
      "   ('question', 0.014274981217129978),\n",
      "   ('claim', 0.01419984973703982),\n",
      "   ('argument', 0.013749060856498872),\n",
      "   ('reason', 0.013636363636363636),\n",
      "   ('evidence', 0.013429752066115703),\n",
      "   ('true', 0.013035311795642374),\n",
      "   ('science', 0.009898572501878287),\n",
      "   ('atheist', 0.009391435011269721),\n",
      "   ('human', 0.009335086401202104)]),\n",
      " (17,\n",
      "  [('write', 0.23300657431321906),\n",
      "   ('article', 0.16409368396337168),\n",
      "   ('line', 0.15361587227048604),\n",
      "   ('host', 0.06445174923691008),\n",
      "   ('organization', 0.059843860061047194),\n",
      "   ('nntp_poste', 0.022247006339516318),\n",
      "   ('reply', 0.020163183845973234),\n",
      "   ('hear', 0.014175862878610002),\n",
      "   ('opinion', 0.008247241136417),\n",
      "   ('news', 0.00810049307349143)]),\n",
      " (18,\n",
      "  [('food', 0.010752688172043012),\n",
      "   ('effect', 0.009944215377152558),\n",
      "   ('problem', 0.009836419004500498),\n",
      "   ('doctor', 0.009135742582262107),\n",
      "   ('day', 0.008704557091653867),\n",
      "   ('time', 0.008273371601045625),\n",
      "   ('patient', 0.00792303338992643),\n",
      "   ('study', 0.007842186110437384),\n",
      "   ('eat', 0.007815237017274369),\n",
      "   ('week', 0.007437949712992158)]),\n",
      " (19,\n",
      "  [('car', 0.045132120424518085),\n",
      "   ('good', 0.039473684210526314),\n",
      "   ('buy', 0.026424084903617067),\n",
      "   ('pay', 0.018193632228719947),\n",
      "   ('make', 0.016623348494693525),\n",
      "   ('write', 0.01556746805284817),\n",
      "   ('cost', 0.01472817847086853),\n",
      "   ('line', 0.013293264024258176),\n",
      "   ('money', 0.012670565302144249),\n",
      "   ('article', 0.012508122157244964)]),\n",
      " (20,\n",
      "  [('people', 0.020398501502614137),\n",
      "   ('happen', 0.018298958461981804),\n",
      "   ('start', 0.014593882507924746),\n",
      "   ('kill', 0.01442921246552221),\n",
      "   ('time', 0.013708781030011115),\n",
      "   ('day', 0.013420608455806678),\n",
      "   ('fire', 0.012391420690790828),\n",
      "   ('leave', 0.0106623852455642),\n",
      "   ('hear', 0.010538882713762299),\n",
      "   ('live', 0.00936560866164423)])]\n"
     ]
    }
   ],
   "source": [
    "# запуск на оптимальном числе топиков\n",
    "all_topics = ldamallet.show_topics(formatted=False, num_topics=21)\n",
    "pprint(all_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_topic(all_topics, text_number):\n",
    "    weights = {}\n",
    "    freqs = {}\n",
    "    for id, freq in corpus[text_number]: # беру уже готовые частотности\n",
    "        freqs[id2word[id]] = freq\n",
    "    for topic in all_topics: # для каждого из топиков\n",
    "        weight = 0 # начинаем считать веса встретившихся слов конкретного топика\n",
    "        for pair in topic[1]: # для каждой пары слово-коэф\n",
    "            if pair[0] in freqs: # если слово есть в тексте\n",
    "                weight += pair[1] * freqs[pair[0]]# коэф * кол-во этого слова в тексте\n",
    "        weights[topic[0]] = weight # айди топика: все веса всех его слов\n",
    "        main = [max(weights, key=weights.get), max(weights.values())]\n",
    "    return main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19, 0.2555772146415421]\n"
     ]
    }
   ],
   "source": [
    "pprint(main_topic(all_topics, 0)) # какой топик самый топовый в первом тексте"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Создание первого датафрейма"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def first_df():\n",
    "    df = pd.DataFrame()\n",
    "    for i in range(len(corpus)):\n",
    "        main = main_topic(all_topics, i)\n",
    "        words = ', '.join(list(map(lambda x: (x[0]), all_topics[main[0]][1])))\n",
    "        items = {'№ текста': i,\n",
    "                'Главный топик': main[0],\n",
    "                'Слова топика': words,\n",
    "                'Вес топика': main[1]}\n",
    "        df = df.append(items, ignore_index=True)\n",
    "    df = df.reindex(columns=['№ текста', 'Главный топик', 'Слова топика', 'Вес топика'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>№ текста</th>\n",
       "      <th>Главный топик</th>\n",
       "      <th>Слова топика</th>\n",
       "      <th>Вес топика</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>car, good, buy, pay, make, write, cost, line, money, article</td>\n",
       "      <td>0.255577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>write, article, line, host, organization, nntp_poste, reply, hear, opinion, news</td>\n",
       "      <td>0.218068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>write, article, line, host, organization, nntp_poste, reply, hear, opinion, news</td>\n",
       "      <td>0.374354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>write, article, line, host, organization, nntp_poste, reply, hear, opinion, news</td>\n",
       "      <td>0.848174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>write, article, line, host, organization, nntp_poste, reply, hear, opinion, news</td>\n",
       "      <td>0.397100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>write, article, line, host, organization, nntp_poste, reply, hear, opinion, news</td>\n",
       "      <td>2.045345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>write, article, line, host, organization, nntp_poste, reply, hear, opinion, news</td>\n",
       "      <td>0.153616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>write, article, line, host, organization, nntp_poste, reply, hear, opinion, news</td>\n",
       "      <td>0.779262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>write, article, line, host, organization, nntp_poste, reply, hear, opinion, news</td>\n",
       "      <td>0.153616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>write, article, line, host, organization, nntp_poste, reply, hear, opinion, news</td>\n",
       "      <td>0.783723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   № текста  Главный топик  \\\n",
       "0  0.0       19.0            \n",
       "1  1.0       17.0            \n",
       "2  2.0       17.0            \n",
       "3  3.0       17.0            \n",
       "4  4.0       17.0            \n",
       "5  5.0       17.0            \n",
       "6  6.0       17.0            \n",
       "7  7.0       17.0            \n",
       "8  8.0       17.0            \n",
       "9  9.0       17.0            \n",
       "\n",
       "                                                                       Слова топика  \\\n",
       "0  car, good, buy, pay, make, write, cost, line, money, article                       \n",
       "1  write, article, line, host, organization, nntp_poste, reply, hear, opinion, news   \n",
       "2  write, article, line, host, organization, nntp_poste, reply, hear, opinion, news   \n",
       "3  write, article, line, host, organization, nntp_poste, reply, hear, opinion, news   \n",
       "4  write, article, line, host, organization, nntp_poste, reply, hear, opinion, news   \n",
       "5  write, article, line, host, organization, nntp_poste, reply, hear, opinion, news   \n",
       "6  write, article, line, host, organization, nntp_poste, reply, hear, opinion, news   \n",
       "7  write, article, line, host, organization, nntp_poste, reply, hear, opinion, news   \n",
       "8  write, article, line, host, organization, nntp_poste, reply, hear, opinion, news   \n",
       "9  write, article, line, host, organization, nntp_poste, reply, hear, opinion, news   \n",
       "\n",
       "   Вес топика  \n",
       "0  0.255577    \n",
       "1  0.218068    \n",
       "2  0.374354    \n",
       "3  0.848174    \n",
       "4  0.397100    \n",
       "5  2.045345    \n",
       "6  0.153616    \n",
       "7  0.779262    \n",
       "8  0.153616    \n",
       "9  0.783723    "
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = first_df()\n",
    "pd.set_option('display.max_colwidth', -1) \n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Расчёт tf / idf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# собирает группы текстов (по их id) с одинаковым главным топиком\n",
    "groups = []\n",
    "for num in range(21):\n",
    "    top = df.loc[df['Главный топик'] == num]\n",
    "    groups.append(top['№ текста'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = [] # список строк из лемм = весь корпус\n",
    "for d in data_lemmatized:\n",
    "    joined.append(' '.join(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_tf_idf_words(vector, feature_names, top_n):\n",
    "    sorted_nzs = np.argsort(vector.data)[:-(top_n+1):-1]\n",
    "    return feature_names[vector.indices[sorted_nzs]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tfidf(group): # для одной группы\n",
    "    test_n = 0\n",
    "    tfidf = {}\n",
    "    group_joined = [] # тут список лем-х текстов одной группы\n",
    "    for g in group: \n",
    "        group_joined.append(joined[int(g)])\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectors = vectorizer.fit_transform(group_joined)\n",
    "    feature_names = np.array(vectorizer.get_feature_names())\n",
    "    for i, (g, post) in enumerate(zip(group, group_joined)):\n",
    "        response = vectorizer.transform([post])\n",
    "        top_words = get_top_tf_idf_words(response, feature_names, 5)\n",
    "        tfidf[g] = ', '.join(top_words)\n",
    "    return tfidf # словарь айди: слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_column = {}\n",
    "for group in groups:\n",
    "    new_column.update(count_tfidf(group)) # добавить к общему результаты одной группы\n",
    "# отсортировать и преврать в список для добавления в df:\n",
    "new_c_list = list(map(lambda x: (x[1]), sorted(new_column.items(), key=lambda x: x[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Топ-5 tf / idf'] = new_c_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>№ текста</th>\n",
       "      <th>Главный топик</th>\n",
       "      <th>Слова топика</th>\n",
       "      <th>Вес топика</th>\n",
       "      <th>Топ-5 tf / idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>car, good, buy, pay, make, write, cost, line, money, article</td>\n",
       "      <td>0.255577</td>\n",
       "      <td>car, door, lerxst, bricklin, where</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>write, article, line, host, organization, nntp_poste, reply, hear, opinion, news</td>\n",
       "      <td>0.218068</td>\n",
       "      <td>poll, clock, upgrade, final, detailing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>write, article, line, host, organization, nntp_poste, reply, hear, opinion, news</td>\n",
       "      <td>0.374354</td>\n",
       "      <td>powerbook, display, machine, bunch, store</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>write, article, line, host, organization, nntp_poste, reply, hear, opinion, news</td>\n",
       "      <td>0.848174</td>\n",
       "      <td>division, quadrilateral, chip, weitek, winter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>write, article, line, host, organization, nntp_poste, reply, hear, opinion, news</td>\n",
       "      <td>0.397100</td>\n",
       "      <td>error, warn, bug, waivere, memory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>write, article, line, host, organization, nntp_poste, reply, hear, opinion, news</td>\n",
       "      <td>2.045345</td>\n",
       "      <td>weapon, needless, individual, tavare, keep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>write, article, line, host, organization, nntp_poste, reply, hear, opinion, news</td>\n",
       "      <td>0.153616</td>\n",
       "      <td>treatment, astrocytoma, thank, tumor, accidentally</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>write, article, line, host, organization, nntp_poste, reply, hear, opinion, news</td>\n",
       "      <td>0.779262</td>\n",
       "      <td>scsi, chip, range, esdi, bit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>write, article, line, host, organization, nntp_poste, reply, hear, opinion, news</td>\n",
       "      <td>0.153616</td>\n",
       "      <td>icon, win, wallpaper, bmp, appreciated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>write, article, line, host, organization, nntp_poste, reply, hear, opinion, news</td>\n",
       "      <td>0.783723</td>\n",
       "      <td>board, diskdoubler, autodoubler, sigma_design, licensing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   № текста  Главный топик  \\\n",
       "0  0.0       19.0            \n",
       "1  1.0       17.0            \n",
       "2  2.0       17.0            \n",
       "3  3.0       17.0            \n",
       "4  4.0       17.0            \n",
       "5  5.0       17.0            \n",
       "6  6.0       17.0            \n",
       "7  7.0       17.0            \n",
       "8  8.0       17.0            \n",
       "9  9.0       17.0            \n",
       "\n",
       "                                                                       Слова топика  \\\n",
       "0  car, good, buy, pay, make, write, cost, line, money, article                       \n",
       "1  write, article, line, host, organization, nntp_poste, reply, hear, opinion, news   \n",
       "2  write, article, line, host, organization, nntp_poste, reply, hear, opinion, news   \n",
       "3  write, article, line, host, organization, nntp_poste, reply, hear, opinion, news   \n",
       "4  write, article, line, host, organization, nntp_poste, reply, hear, opinion, news   \n",
       "5  write, article, line, host, organization, nntp_poste, reply, hear, opinion, news   \n",
       "6  write, article, line, host, organization, nntp_poste, reply, hear, opinion, news   \n",
       "7  write, article, line, host, organization, nntp_poste, reply, hear, opinion, news   \n",
       "8  write, article, line, host, organization, nntp_poste, reply, hear, opinion, news   \n",
       "9  write, article, line, host, organization, nntp_poste, reply, hear, opinion, news   \n",
       "\n",
       "   Вес топика                                            Топ-5 tf / idf  \n",
       "0  0.255577    car, door, lerxst, bricklin, where                        \n",
       "1  0.218068    poll, clock, upgrade, final, detailing                    \n",
       "2  0.374354    powerbook, display, machine, bunch, store                 \n",
       "3  0.848174    division, quadrilateral, chip, weitek, winter             \n",
       "4  0.397100    error, warn, bug, waivere, memory                         \n",
       "5  2.045345    weapon, needless, individual, tavare, keep                \n",
       "6  0.153616    treatment, astrocytoma, thank, tumor, accidentally        \n",
       "7  0.779262    scsi, chip, range, esdi, bit                              \n",
       "8  0.153616    icon, win, wallpaper, bmp, appreciated                    \n",
       "9  0.783723    board, diskdoubler, autodoubler, sigma_design, licensing  "
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно на датафрейме, у очень большого количества текстов после подсчёта весов слов, входящих в топик, главным топиком оказался топик №17. Это объясняется тем, что в 17 топике сразу трём словам маллет определяет очень большой вес: article, write и line (см. ниже). Если хотя бы одно из этих слов встретится в тексте, то почти навверняка перевесит все остальные слова, т.к. им маллет приписал веса в разы меньшие. Если в тексте встретятся два тяжёлых слова, то у остальных топиков просто нет шансов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17,\n",
      " [('write', 0.23300657431321906),\n",
      "  ('article', 0.16409368396337168),\n",
      "  ('line', 0.15361587227048604),\n",
      "  ('host', 0.06445174923691008),\n",
      "  ('organization', 0.059843860061047194),\n",
      "  ('nntp_poste', 0.022247006339516318),\n",
      "  ('reply', 0.020163183845973234),\n",
      "  ('hear', 0.014175862878610002),\n",
      "  ('opinion', 0.008247241136417),\n",
      "  ('news', 0.00810049307349143)])\n"
     ]
    }
   ],
   "source": [
    "pprint(all_topics[17]) # топик с тяжелыми словами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15,\n",
      " [('key', 0.04125268989515132),\n",
      "  ('system', 0.015841765486928255),\n",
      "  ('encryption', 0.01524655464493384),\n",
      "  ('chip', 0.01307174579918502),\n",
      "  ('public', 0.01124032782381759),\n",
      "  ('government', 0.011194542374433404),\n",
      "  ('bit', 0.011125864200357126),\n",
      "  ('message', 0.011034293301588755),\n",
      "  ('technology', 0.01091982967812829),\n",
      "  ('security', 0.010690902431207362)])\n"
     ]
    }
   ],
   "source": [
    "pprint(all_topics[15]) # пример другого топика, где тяжелых слов нет"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Что такое Coherence score:  \n",
    "Измеряет относительную близость слов внутри одного топика. 0 - близости нет, 1 - максимальная близкость (т.е. это вообще одно и то же слово)  \n",
    "\n",
    "Пайплайн, состоящий из:  \n",
    "* Segmentation - разделение топиков  \n",
    "* Probability Estimation - количественное вычисление вероятностей относительно reference corpus  \n",
    "* Confirmation Measure - преобразование этих вероятностей   \n",
    "* Aggregation - обобщение данных в одно финальное число   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DZ3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
